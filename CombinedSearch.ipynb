{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZtPSfKgIdzJsRNdqCR/vO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seansphd/CombinedSearch/blob/main/CombinedSearch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPwSNV6CqXws"
      },
      "outputs": [],
      "source": [
        "# Colab Notebook: JSON Knowledge Base Chatbot with Live Editable System Prompt\n",
        "#\n",
        "# This notebook builds a lightweight QA tool over a JSON or JSON-LD knowledge base.\n",
        "# You can steer behaviour with a system prompt and an optional bias theme.\n",
        "# The tool pulls in text from linked PDFs, selected by simple keyword scoring and embedding similarity.\n",
        "\n",
        "# When run you will need to enter an OpenAI API key that can be provided for testing purposes\n",
        "# It will then ask you to load the json knowledge base\n",
        "\n",
        "# The system prompt does not need to be changed. You can edit it if for instance you want the system to be biased towards \"Cybernetics\".\n",
        "# This can be added in plain text\n",
        "\n",
        "# When you ask the question you will recieve a few responses.. these will include infrormation about the pdfs selected for use through keyword and semantic search.\n",
        "# You will then recieve the chatr response that is based on the selected PDFs and the system prompt\n",
        "\n",
        "# üõ†Ô∏è Install dependencies\n",
        "# Note: Colab supports shell commands with a leading !. This installs all Python packages used below.\n",
        "!pip install -q openai PyMuPDF requests numpy scikit-learn psutil ipywidgets pillow pdf2image pytesseract\n",
        "\n",
        "# üì¶ Imports\n",
        "import os\n",
        "import json\n",
        "import fitz  # PyMuPDF (fast PDF text extraction)\n",
        "import requests\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from openai import OpenAI  # Official OpenAI Python SDK\n",
        "from google.colab import files  # File upload dialog in Colab\n",
        "from IPython.display import display, HTML, clear_output  # UI helpers\n",
        "import ipywidgets as widgets  # Interactive controls\n",
        "import psutil  # Process and memory information\n",
        "import re\n",
        "import html\n",
        "from functools import lru_cache  # In-memory caching for repeated PDF fetches\n",
        "\n",
        "# Enable widgets in Colab\n",
        "# The custom widget manager is needed for certain ipywidgets to render correctly in Colab.\n",
        "try:\n",
        "    from google.colab import output as colab_output\n",
        "    colab_output.enable_custom_widget_manager()\n",
        "except Exception:\n",
        "    # Safe to ignore if unavailable\n",
        "    pass\n",
        "\n",
        "# ----------------------------\n",
        "# Global state\n",
        "# ----------------------------\n",
        "chat_history_html = \"\"  # Accumulates HTML content of the chat for display\n",
        "api_key_ready = False   # Tracks whether the API key has been entered\n",
        "\n",
        "# ----------------------------\n",
        "# API key UI\n",
        "# ----------------------------\n",
        "# Password widget hides input while typing.\n",
        "api_key_input = widgets.Password(\n",
        "    description='API Key:',\n",
        "    placeholder='Enter your OpenAI API key',\n",
        "    layout=widgets.Layout(width='50%')\n",
        ")\n",
        "# Button to save the key into the environment.\n",
        "submit_button = widgets.Button(description='Save API Key', button_style='info')\n",
        "# Output area to print status messages without cluttering the notebook.\n",
        "output_area = widgets.Output()\n",
        "\n",
        "def set_api_key(_):\n",
        "    \"\"\"Save API key into environment and mark readiness.\"\"\"\n",
        "    global api_key_ready\n",
        "    with output_area:\n",
        "        clear_output()\n",
        "        if api_key_input.value.strip() == \"\":\n",
        "            # Basic validation to avoid empty keys.\n",
        "            display(HTML(\"<b style='color:#e53935;'>‚ùå Please enter a valid API key.</b>\"))\n",
        "        else:\n",
        "            os.environ[\"OPENAI_API_KEY\"] = api_key_input.value.strip()\n",
        "            api_key_ready = True\n",
        "            display(HTML(\"<b style='color:#43a047;'>‚úÖ API key saved. You can continue.</b>\"))\n",
        "\n",
        "# Wire up the click handler and render the input UI.\n",
        "submit_button.on_click(set_api_key)\n",
        "display(HTML(\"<h3>üîë Enter your OpenAI API key to continue:</h3>\"))\n",
        "display(api_key_input, submit_button, output_area)\n",
        "\n",
        "# ----------------------------\n",
        "# OpenAI client\n",
        "# ----------------------------\n",
        "def get_openai_client():\n",
        "    \"\"\"\n",
        "    Return an OpenAI client instance.\n",
        "    Priority for key:\n",
        "      1) Existing environment variable (already saved),\n",
        "      2) Current widget value (if present).\n",
        "    Raises a clear error if missing.\n",
        "    \"\"\"\n",
        "    key = os.getenv(\"OPENAI_API_KEY\")\n",
        "    try:\n",
        "        if (not key) and api_key_input.value:\n",
        "            key = api_key_input.value.strip()\n",
        "            if key:\n",
        "                os.environ[\"OPENAI_API_KEY\"] = key\n",
        "    except NameError:\n",
        "        # api_key_input can be missing in some contexts\n",
        "        pass\n",
        "    if not key:\n",
        "        raise ValueError(\"API key not set. Please enter your API key and try again.\")\n",
        "    return OpenAI(api_key=key)\n",
        "\n",
        "# ----------------------------\n",
        "# JSON loader with JSON-LD support\n",
        "# ----------------------------\n",
        "def normalise_to_list(obj):\n",
        "    \"\"\"\n",
        "    Accepts:\n",
        "      - A list of entries,\n",
        "      - A dict holding a list under common JSON-LD or container keys,\n",
        "      - A single entry dict.\n",
        "    Returns a list of entries for uniform downstream handling.\n",
        "    \"\"\"\n",
        "    if isinstance(obj, list):\n",
        "        return obj\n",
        "    if isinstance(obj, dict):\n",
        "        # Common containers in JSON-LD and ad-hoc exports.\n",
        "        for k in [\"@graph\", \"graph\", \"data\", \"items\", \"entries\", \"documents\", \"records\"]:\n",
        "            if k in obj and isinstance(obj[k], list):\n",
        "                return obj[k]\n",
        "        # Treat as a single entry if no container is found.\n",
        "        return [obj]\n",
        "    raise ValueError(\"Unsupported JSON structure\")\n",
        "\n",
        "def upload_json():\n",
        "    \"\"\"\n",
        "    Prompt the user to upload a JSON or JSON-LD file.\n",
        "    Normalise it to a list and report entry count.\n",
        "    \"\"\"\n",
        "    display(HTML(\"<h3>üì• Upload your JSON knowledge base file...</h3>\"))\n",
        "    uploaded = files.upload()  # Opens a file chooser in Colab\n",
        "    if not uploaded:\n",
        "        raise ValueError(\"No file uploaded.\")\n",
        "    # Take the first uploaded file\n",
        "    json_file_path = next(iter(uploaded))\n",
        "    with open(json_file_path, \"r\") as f:\n",
        "        kb_raw = json.load(f)\n",
        "    kb = normalise_to_list(kb_raw)\n",
        "    display(HTML(f\"<b>‚úÖ Loaded {len(kb)} entries from {html.escape(json_file_path)}</b>\"))\n",
        "    return kb\n",
        "\n",
        "# ----------------------------\n",
        "# Summaries and embeddings\n",
        "# ----------------------------\n",
        "def entry_summary(entry):\n",
        "    \"\"\"\n",
        "    Produce a brief text summary for an entry.\n",
        "    Preference order:\n",
        "      1) Existing summary fields,\n",
        "      2) Concise metadata concatenation,\n",
        "      3) JSON string fallback (truncated).\n",
        "    \"\"\"\n",
        "    for k in [\"summary\", \"abstract\", \"description\"]:\n",
        "        if isinstance(entry.get(k), str) and entry[k].strip():\n",
        "            return entry[k]\n",
        "    parts = []\n",
        "    for k in [\"title\", \"date\", \"datePublished\", \"author\", \"keywords\"]:\n",
        "        v = entry.get(k)\n",
        "        if isinstance(v, list):\n",
        "            parts.append(\" \".join(map(str, v)))\n",
        "        elif isinstance(v, str):\n",
        "            parts.append(v)\n",
        "    if not parts:\n",
        "        # Fall back to compact JSON if no obvious human fields\n",
        "        return json.dumps(entry)[:1000]\n",
        "    return \" | \".join(parts)\n",
        "\n",
        "def create_embeddings(kb):\n",
        "    \"\"\"\n",
        "    Create embeddings for each entry summary.\n",
        "    Uses text-embedding-3-small for cost-effective vectors.\n",
        "    Returns:\n",
        "      summaries: list[str]\n",
        "      summary_embeddings: np.ndarray shape (N, D)\n",
        "    \"\"\"\n",
        "    client = get_openai_client()\n",
        "    display(HTML(\"<b>üîÑ Generating embeddings for entries...</b>\"))\n",
        "\n",
        "    def get_embedding(text):\n",
        "        # The API expects a string input and returns a single vector for each input string.\n",
        "        resp = client.embeddings.create(\n",
        "            model=\"text-embedding-3-small\",\n",
        "            input=text if text else \"\"\n",
        "        )\n",
        "        return np.array(resp.data[0].embedding, dtype=np.float32)\n",
        "\n",
        "    summaries = [entry_summary(e) for e in kb]\n",
        "    # Stack into an array of shape (num_entries, embedding_dim)\n",
        "    summary_embeddings = np.stack([get_embedding(s) for s in summaries], axis=0)\n",
        "    display(HTML(\"<b>‚úÖ Embeddings created.</b>\"))\n",
        "    return summaries, summary_embeddings\n",
        "\n",
        "# ----------------------------\n",
        "# PDF fetch with cache and optional OCR\n",
        "# ----------------------------\n",
        "def maybe_install_ocr_deps():\n",
        "    \"\"\"\n",
        "    Install system packages used for OCR on demand:\n",
        "      - poppler-utils provides pdftoppm for rasterisation,\n",
        "      - tesseract-ocr performs OCR.\n",
        "    Runs only if missing to save time.\n",
        "    \"\"\"\n",
        "    import shutil\n",
        "    needs_poppler = shutil.which(\"pdftoppm\") is None\n",
        "    needs_tesseract = shutil.which(\"tesseract\") is None\n",
        "    if needs_poppler or needs_tesseract:\n",
        "        print(\"Installing OCR system packages. This may take a minute...\")\n",
        "        # Quiet apt to avoid excessive logs\n",
        "        os.system(\"apt-get update -y >/dev/null 2>&1\")\n",
        "        if needs_poppler:\n",
        "            os.system(\"apt-get install -y poppler-utils >/dev/null 2>&1\")\n",
        "        if needs_tesseract:\n",
        "            os.system(\"apt-get install -y tesseract-ocr >/dev/null 2>&1\")\n",
        "\n",
        "@lru_cache(maxsize=256)\n",
        "def fetch_pdf_bytes(url):\n",
        "    \"\"\"\n",
        "    Download PDF bytes with a small cache to avoid repeated network calls.\n",
        "    GitHub 'blob' URLs are converted to their raw counterparts.\n",
        "    \"\"\"\n",
        "    if not url:\n",
        "        raise ValueError(\"Empty URL\")\n",
        "    raw_url = url.replace(\"github.com\", \"raw.githubusercontent.com\").replace(\"/blob\", \"\")\n",
        "    resp = requests.get(raw_url, timeout=60)\n",
        "    resp.raise_for_status()\n",
        "    return resp.content\n",
        "\n",
        "def extract_text_pymupdf(pdf_bytes):\n",
        "    \"\"\"\n",
        "    Extract text using PyMuPDF. Works well for digital PDFs.\n",
        "    Returns an empty string when no extractable text exists.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        text = \"\"\n",
        "        with fitz.open(stream=pdf_bytes, filetype=\"pdf\") as pdf_doc:\n",
        "            for page in pdf_doc:\n",
        "                # get_text returns a plain string per page\n",
        "                text += page.get_text() or \"\"\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        # Keep errors readable in the UI\n",
        "        return f\"[PDF parse error: {e}]\"\n",
        "\n",
        "def ocr_pdf_bytes(pdf_bytes):\n",
        "    \"\"\"\n",
        "    Fallback OCR path:\n",
        "      1) Convert pages to images with pdf2image,\n",
        "      2) Run Tesseract to recognise text,\n",
        "      3) Join page texts.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from pdf2image import convert_from_bytes\n",
        "        import pytesseract\n",
        "        pages = convert_from_bytes(pdf_bytes, fmt=\"png\", dpi=200)\n",
        "        texts = []\n",
        "        for img in pages:\n",
        "            texts.append(pytesseract.image_to_string(img))\n",
        "        return \"\\n\".join(texts)\n",
        "    except Exception as e:\n",
        "        return f\"[OCR error: {e}]\"\n",
        "\n",
        "def fetch_pdf_text(url, try_ocr=False):\n",
        "    \"\"\"\n",
        "    Retrieve text from a PDF URL.\n",
        "    If try_ocr is True and extracted text is very short, attempt OCR.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        b = fetch_pdf_bytes(url)\n",
        "        text = extract_text_pymupdf(b)\n",
        "        if try_ocr:\n",
        "            # Heuristic to decide whether OCR might help\n",
        "            if len(text.strip()) < 100:\n",
        "                ocr_text = ocr_pdf_bytes(b)\n",
        "                if len(ocr_text.strip()) > len(text.strip()):\n",
        "                    return ocr_text\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        return f\"[PDF fetch error: {e}]\"\n",
        "\n",
        "# ----------------------------\n",
        "# Helpers\n",
        "# ----------------------------\n",
        "# Minimal stopword list for simple entity extraction. This keeps logic transparent.\n",
        "stopwords = set(\"the and is it to for in of on with a an as at by from this that\".split())\n",
        "\n",
        "def extract_entities(text):\n",
        "    \"\"\"\n",
        "    Tokenise to words and drop stopwords and short tokens.\n",
        "    This is a crude keyword extractor suited to quick scoring.\n",
        "    \"\"\"\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    return [w for w in words if w not in stopwords and len(w) > 2]\n",
        "\n",
        "def keyword_score(entry, entities):\n",
        "    \"\"\"\n",
        "    Score an entry by counting occurrences of entity tokens\n",
        "    across all field values concatenated into one string.\n",
        "    \"\"\"\n",
        "    combined = \" \".join(str(entry.get(field, \"\")) for field in entry).lower()\n",
        "    return sum(combined.count(e) for e in entities)\n",
        "\n",
        "def print_memory_usage(conversation_history):\n",
        "    \"\"\"\n",
        "    Show process RAM, system RAM usage in Colab, and chat history size.\n",
        "    Helps diagnose out-of-memory issues.\n",
        "    \"\"\"\n",
        "    process = psutil.Process(os.getpid())\n",
        "    ram_used_mb = process.memory_info().rss / (1024 * 1024)\n",
        "    vmem = psutil.virtual_memory()\n",
        "    colab_ram = vmem.used / (1024 * 1024)\n",
        "    colab_total_ram = vmem.total / (1024 * 1024)\n",
        "    display(HTML(f\"\"\"\n",
        "        <div style='border:1px solid #444; padding:6px; margin:6px; border-radius:6px; background:#222; color:#ccc; font-family:monospace;'>\n",
        "        üß† <b>Memory Stats:</b><br>\n",
        "        üîπ Process RAM: {ram_used_mb:.2f} MB<br>\n",
        "        üîπ Colab RAM: {colab_ram:.2f} MB / {colab_total_ram:.2f} MB<br>\n",
        "        üîπ Chat History Size: {len(json.dumps(conversation_history))/1024:.1f} KB\n",
        "        </div>\n",
        "    \"\"\"))\n",
        "\n",
        "# ----------------------------\n",
        "# Chat UI\n",
        "# ----------------------------\n",
        "def chat_ui(kb, summaries, summary_embeddings):\n",
        "    \"\"\"\n",
        "    Build and run the interactive chat UI:\n",
        "      - Choose bias and system prompt,\n",
        "      - Pick relevant entries,\n",
        "      - Pull PDF text context,\n",
        "      - Call the Chat Completions API,\n",
        "      - Render a running transcript.\n",
        "    \"\"\"\n",
        "    global chat_history_html\n",
        "    client = get_openai_client()\n",
        "\n",
        "    # Controls\n",
        "    base_system_prompt = \"You are a helpful assistant with access to a structured JSON knowledge base and linked PDFs.\"\n",
        "    bias_input = widgets.Text(\n",
        "        value=\"\",\n",
        "        placeholder='Optional bias theme, e.g. Cybernetics',\n",
        "        description='Bias:',\n",
        "        layout=widgets.Layout(width='100%')\n",
        "    )\n",
        "    system_prompt_input = widgets.Textarea(\n",
        "        value=base_system_prompt,\n",
        "        placeholder='Enter the base system prompt here...',\n",
        "        description='System Prompt:',\n",
        "        layout=widgets.Layout(width='100%', height='80px')\n",
        "    )\n",
        "    # Top K entries to include as context\n",
        "    top_k_slider = widgets.IntSlider(value=2, min=1, max=10, step=1, description='Top K:')\n",
        "    # Balance between embeddings and keyword hits\n",
        "    kw_weight_slider = widgets.FloatSlider(value=0.5, min=0.0, max=2.0, step=0.1, description='Keyword weight:')\n",
        "    # Max characters per document context included in the prompt\n",
        "    ctx_chars_slider = widgets.IntSlider(value=6000, min=1000, max=20000, step=500, description='Chars/doc:')\n",
        "    # OCR toggle for scanned PDFs that lack extractable text\n",
        "    ocr_checkbox = widgets.Checkbox(value=False, description='Use OCR fallback for scanned PDFs')\n",
        "    # Optional memory stats after each reply to aid debugging\n",
        "    show_mem_checkbox = widgets.Checkbox(value=False, description='Show memory stats after replies')\n",
        "\n",
        "    # State\n",
        "    chat_messages = []  # Conversation turns for the Chat Completions API\n",
        "    query_embed_cache = {}  # Avoid recomputing embeddings for repeated queries\n",
        "\n",
        "    def get_embedding(text):\n",
        "        \"\"\"\n",
        "        Cache embeddings per unique query prefix to save tokens and time.\n",
        "        \"\"\"\n",
        "        key = text[:2000]\n",
        "        if key in query_embed_cache:\n",
        "            return query_embed_cache[key]\n",
        "        resp = client.embeddings.create(model=\"text-embedding-3-small\", input=text if text else \"\")\n",
        "        arr = np.array(resp.data[0].embedding, dtype=np.float32)\n",
        "        query_embed_cache[key] = arr\n",
        "        return arr\n",
        "\n",
        "    def current_system_prompt():\n",
        "        \"\"\"\n",
        "        Combine base system prompt and optional bias theme.\n",
        "        The bias is appended as plain text for transparency.\n",
        "        \"\"\"\n",
        "        bias = bias_input.value.strip()\n",
        "        if bias:\n",
        "            return f\"{system_prompt_input.value}\\nBias theme: {bias}\"\n",
        "        return system_prompt_input.value\n",
        "\n",
        "    def update_chat_display():\n",
        "        \"\"\"\n",
        "        Render the fixed system prompt banner and scrollable chat area.\n",
        "        \"\"\"\n",
        "        safe_prompt = html.escape(current_system_prompt())\n",
        "        display(HTML(f\"\"\"\n",
        "        <div style='position:sticky; top:0; background:#222; color:#fff; padding:8px; border-bottom:2px solid #555; z-index:10;'>\n",
        "          üìù <b>System Prompt:</b> {safe_prompt} | üîë <b>API Key:</b> ‚úÖ Active\n",
        "        </div>\n",
        "        <div style='max-height:420px; overflow-y:auto; border:1px solid #444; background:#111; color:#ddd; padding:10px; border-radius:8px;'>\n",
        "          {chat_history_html}\n",
        "        </div>\n",
        "        \"\"\"))\n",
        "\n",
        "    def pick_top_entries(user_text, top_k, kw_weight):\n",
        "        \"\"\"\n",
        "        Rank entries by:\n",
        "          cosine_similarity(embedding(query), embedding(summary))\n",
        "          plus a weighted keyword score.\n",
        "        Return indices and the combined score vector.\n",
        "        \"\"\"\n",
        "        q_emb = get_embedding(user_text)\n",
        "        sims = cosine_similarity([q_emb], summary_embeddings)[0]\n",
        "        ents = extract_entities(user_text)\n",
        "        kw_scores = np.array([keyword_score(e, ents) for e in kb], dtype=np.float32)\n",
        "        # Normalise keyword scores by max to avoid scaling issues\n",
        "        combined = sims + kw_weight * (kw_scores / (kw_scores.max() if kw_scores.max() > 0 else 1.0))\n",
        "        idxs = np.argsort(combined)[-top_k:][::-1]\n",
        "        return [idx for idx in idxs], combined\n",
        "\n",
        "    def safe_html(s):\n",
        "        \"\"\"\n",
        "        Escape HTML and convert newlines to <br> for display.\n",
        "        \"\"\"\n",
        "        return html.escape(s).replace(\"\\n\", \"<br>\")\n",
        "\n",
        "    def on_send(_):\n",
        "        \"\"\"\n",
        "        Handle a user query:\n",
        "          - Optionally install OCR tools,\n",
        "          - Select top entries,\n",
        "          - Fetch and truncate PDF texts,\n",
        "          - Call the chat model,\n",
        "          - Update the transcript and optional memory stats.\n",
        "        \"\"\"\n",
        "        global chat_history_html\n",
        "        user_input = text_input.value.strip()\n",
        "        text_input.value = \"\"\n",
        "        if not user_input:\n",
        "            return\n",
        "\n",
        "        if ocr_checkbox.value:\n",
        "            maybe_install_ocr_deps()\n",
        "\n",
        "        top_k = top_k_slider.value\n",
        "        kw_w = kw_weight_slider.value\n",
        "        ctx_chars = ctx_chars_slider.value\n",
        "\n",
        "        # Entry selection using combined score\n",
        "        top_idxs, combined = pick_top_entries(user_input, top_k=top_k, kw_weight=kw_w)\n",
        "        selected_entries = [kb[i] for i in top_idxs]\n",
        "\n",
        "        # Show brief cards for each selected entry\n",
        "        for i in top_idxs:\n",
        "            entry = kb[i]\n",
        "            title = entry.get('title') or entry.get('name') or 'Untitled'\n",
        "            date = entry.get('date') or entry.get('datePublished') or ''\n",
        "            url = entry.get('url') or entry.get('pdf') or entry.get('link') or ''\n",
        "            summ = summaries[i][:300] + (\"...\" if len(summaries[i]) > 300 else \"\")\n",
        "            url_html = f\"<a href='{html.escape(url)}' target='_blank' style='color:#1e90ff;'>Open</a>\" if url else \"<i>No link</i>\"\n",
        "            chat_history_html += f\"\"\"\n",
        "            <div style='background:#333; padding:8px; margin:6px 0; border-radius:6px;'>\n",
        "              üìÑ <b>{html.escape(title)}</b> {f\"({html.escape(str(date))})\" if date else \"\"}<br>\n",
        "              <i>{html.escape(summ)}</i><br>\n",
        "              {url_html}\n",
        "            </div>\n",
        "            \"\"\"\n",
        "\n",
        "        # Pull text from linked PDFs and truncate to the per-doc limit\n",
        "        texts = []\n",
        "        for e in selected_entries:\n",
        "            url = e.get('url') or e.get('pdf') or e.get('link') or ''\n",
        "            doc_text = fetch_pdf_text(url, try_ocr=ocr_checkbox.value) if url else \"\"\n",
        "            if isinstance(doc_text, str) and len(doc_text) > ctx_chars:\n",
        "                doc_text = doc_text[:ctx_chars] + f\"\\n[Truncated to {ctx_chars} characters]\"\n",
        "            texts.append(doc_text)\n",
        "\n",
        "        # Concatenate the selected texts with clear separators\n",
        "        combined_pdf_text = \"\\n\\n---\\n\\n\".join(texts)\n",
        "\n",
        "        # Construct a user message that includes the gathered context plainly\n",
        "        user_message = {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Relevant PDF Content:\\n{combined_pdf_text}\\n\\nUser Question: {user_input}\"\n",
        "        }\n",
        "        chat_messages.append(user_message)\n",
        "\n",
        "        # Prepend the current system prompt for each request\n",
        "        conversation_history = [{\"role\": \"system\", \"content\": current_system_prompt()}] + chat_messages\n",
        "\n",
        "        # Call the chat model. Temperature set to 0.7 for balanced output.\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=\"gpt-4o-mini\",\n",
        "                messages=conversation_history,\n",
        "                max_tokens=700,\n",
        "                temperature=0.7,\n",
        "            )\n",
        "            assistant_msg = response.choices[0].message.content\n",
        "        except Exception as e:\n",
        "            # Surface API errors to the UI without stopping the app\n",
        "            assistant_msg = f\"‚ö†Ô∏è API Error: {e}\"\n",
        "\n",
        "        # Append assistant reply to our running state\n",
        "        chat_messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
        "\n",
        "        # Render the assistant reply block\n",
        "        chat_history_html += f\"\"\"\n",
        "        <div style='background:#222; padding:8px; margin:6px 0; border-radius:6px;'>\n",
        "          <b style='color:#ffdd57;'>Assistant:</b><br>{safe_html(assistant_msg)}\n",
        "        </div>\n",
        "        \"\"\"\n",
        "\n",
        "        update_chat_display()\n",
        "        # Optional memory diagnostics\n",
        "        if show_mem_checkbox.value:\n",
        "            print_memory_usage(conversation_history)\n",
        "\n",
        "    # Input field and buttons for the chat loop\n",
        "    text_input = widgets.Text(placeholder='Type your question here...')\n",
        "    send_button = widgets.Button(description='Send', button_style='success')\n",
        "    send_button.on_click(on_send)\n",
        "    clear_button = widgets.Button(description='Clear Chat', button_style='danger')\n",
        "\n",
        "    def on_clear(_):\n",
        "        \"\"\"\n",
        "        Clear the transcript and in-memory message history.\n",
        "        \"\"\"\n",
        "        global chat_history_html\n",
        "        chat_history_html = \"\"\n",
        "        chat_messages.clear()\n",
        "        update_chat_display()\n",
        "    clear_button.on_click(on_clear)\n",
        "\n",
        "    # Group controls together and show the UI\n",
        "    controls = widgets.HBox([top_k_slider, kw_weight_slider, ctx_chars_slider, ocr_checkbox, show_mem_checkbox])\n",
        "    update_chat_display()\n",
        "    display(widgets.VBox([bias_input, system_prompt_input, controls, widgets.HBox([text_input, send_button, clear_button])]))\n",
        "\n",
        "# ----------------------------\n",
        "# Proceed gate\n",
        "# ----------------------------\n",
        "# Separate button to enforce ordering:\n",
        "#  1) Provide API key,\n",
        "#  2) Upload JSON,\n",
        "#  3) Build embeddings,\n",
        "#  4) Open chat UI.\n",
        "proceed_button = widgets.Button(description='Proceed', button_style='primary')\n",
        "proceed_out = widgets.Output()\n",
        "\n",
        "def proceed(_):\n",
        "    \"\"\"\n",
        "    Guarded flow to ensure the API key is present before prompting for a file.\n",
        "    Any exceptions are displayed inline.\n",
        "    \"\"\"\n",
        "    with proceed_out:\n",
        "        clear_output()\n",
        "        if not api_key_ready and not os.getenv(\"OPENAI_API_KEY\"):\n",
        "            display(HTML(\"<b style='color:#e53935;'>‚ùå Please save your API key first.</b>\"))\n",
        "            return\n",
        "        try:\n",
        "            kb = upload_json()\n",
        "            summaries, summary_embeddings = create_embeddings(kb)\n",
        "            chat_ui(kb, summaries, summary_embeddings)\n",
        "        except Exception as e:\n",
        "            display(HTML(f\"<b style='color:#e53935;'>‚ö†Ô∏è Error:</b> {html.escape(str(e))}\"))\n",
        "\n",
        "display(HTML(\"<h3>‚ñ∂Ô∏è When your API key shows as saved, click Proceed:</h3>\"))\n",
        "display(proceed_button, proceed_out)\n",
        "proceed_button.on_click(proceed)\n",
        "\n",
        "\n"
      ]
    }
  ]
}